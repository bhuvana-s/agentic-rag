{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b92f7-72e0-4567-bf8c-4a96618fd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "522b34a0-8ab9-4790-89aa-ddf77835dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toxic Rail Experiment\n",
    "# This is an experiment to classify toxic comments using embeddings and a Random Forest classifier.\n",
    "# The dataset used for this experiment was taken from this [kaggle dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)    \n",
    "# The dataset has 3 types of labels - toxic, severe_toxic, non_toxic.\n",
    "# The objective is to build a classifier that can predict if a comment is toxic, severe_toxic or non_toxic. \n",
    "# This is a binary classification problem.\n",
    "\n",
    "# The notebook covers the following steps:\n",
    "# Training the model\n",
    "# Analyzing feature importance\n",
    "# Visualizing the data using the top 2 most important embedding dimensions\n",
    "# Testing the model with various example texts\n",
    "# The model correctly identifies:\n",
    "#   Toxic/obscene comments as toxic\n",
    "# Normal, non-toxic discussion about classification as non-toxic\n",
    "\n",
    "\n",
    "import pandas as pd # for data manipulation and analysis            \n",
    "import numpy as np # for numerical operations   \n",
    "from llama_index.embeddings.ollama import OllamaEmbedding # for embedding generation\n",
    "from tqdm import tqdm # for progress bar\n",
    "from sklearn.ensemble import RandomForestClassifier # for classification\n",
    "from sklearn.metrics import roc_auc_score # for evaluation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # for evaluation\n",
    "import pickle # for model persistence   \n",
    "\n",
    "\n",
    "# Toxic Rail class\n",
    "# This class is used to train and predict toxicity in comments using embeddings and a Random Forest classifier.\n",
    "class Toxic_Rail:\n",
    "    \"\"\"\n",
    "    A class to handle training and prediction of toxicity in comments using embeddings and a Random Forest classifier.\n",
    "\n",
    "    Attributes:\n",
    "    embed_model (OllamaEmbedding): The embedding model used for generating text embeddings.\n",
    "    model (RandomForestClassifier): The trained Random Forest model for prediction.\n",
    "    df_train (DataFrame): The prepared training data.\n",
    "    \"\"\"\n",
    "\n",
    "    # constructor\n",
    "    # This method initializes the Toxic_Rail class with either training or prediction mode. \n",
    "    def __init__(self, mode, input_path=None): # constructor\n",
    "        \"\"\"\n",
    "        Initializes the Toxic_Rail class with either training or prediction mode.\n",
    "\n",
    "        Args:\n",
    "        mode (str): The mode of operation, either \"train\" or \"predict\".\n",
    "        input_path (str, optional): The path to the input CSV file for training.\n",
    "        \"\"\"\n",
    "        self.embed_model = OllamaEmbedding(model_name=\"mxbai-embed-large:latest\") # embedding model\n",
    "        self.X = pd.DataFrame([])\n",
    "        if mode == \"train\": # train mode\n",
    "            self.X, self.model = self.train(input_path, \"model/clf.mdl\")\n",
    "        if mode == \"predict\": # predict mode\n",
    "            self.model = pickle.load(open(\"model/clf.mdl\", \"rb\"))\n",
    "        if mode == \"evaluate\": # evaluate mode\n",
    "            self.model = pickle.load(open(\"model/clf.mdl\", \"rb\"))\n",
    "            self.evaluate(input_path)\n",
    "\n",
    "    # evaluate method\n",
    "    # This method evaluates the model on a test dataset and returns the accuracy and AUC score. \n",
    "    def evaluate(self, test_path): \n",
    "        tqdm.pandas()\n",
    "        df_test = self.prepare_data(test_path)\n",
    "        embeddings = df_test['comment_text'].progress_apply(self.get_embeddings) # get embeddings   \n",
    "        columns = [\"embed_\" + str(i) for i in range(len(embeddings[0]))] # column names\n",
    "        X_test = pd.DataFrame(embeddings.tolist(), columns=columns) # convert embeddings to dataframe\n",
    "        y_test = df_test['label'] # get labels\n",
    "        score = self.model.score(X_test, y_test) # get accuracy\n",
    "        auc_val = roc_auc_score(y_test, self.model.predict_proba(X_test)[:,1]) # get AUC score\n",
    "        return score, auc_val\n",
    "        \n",
    "    # train method\n",
    "    # This method trains the Random Forest model on the provided dataset and saves the model.   \n",
    "    def train(self, input_path, model_persist_path): \n",
    "        \"\"\"\n",
    "        Trains the Random Forest model on the provided dataset and saves the model.\n",
    "\n",
    "        Args:\n",
    "        input_path (str): The path to the input CSV file for training.\n",
    "        model_persist_path (str): The path where the trained model will be saved.\n",
    "        \"\"\"\n",
    "        tqdm.pandas()\n",
    "        self.df_train = self.prepare_data(input_path)\n",
    "        embeddings = self.df_train['comment_text'].progress_apply(self.get_embeddings)\n",
    "        columns = [\"embed_\" + str(i) for i in range(len(embeddings[0]))]\n",
    "        X = pd.DataFrame(embeddings.tolist(), columns=columns)\n",
    "        y = self.df_train['label']\n",
    "        clf = RandomForestClassifier(max_depth=16, random_state=0).fit(X, y)\n",
    "        print(clf.score(X, y))\n",
    "        pickle.dump(clf, open(model_persist_path, \"wb\"))\n",
    "        return X, clf\n",
    "\n",
    "    # predict method\n",
    "    # This method predicts the toxicity label for a given text using the trained model. \n",
    "    def predict(self, text): \n",
    "        \"\"\"\n",
    "        Predicts the toxicity label for a given text using the trained model.\n",
    "\n",
    "        Args:\n",
    "        text (str): The text to be classified.\n",
    "\n",
    "        Returns:\n",
    "        array: The predicted label for the input text.\n",
    "        \"\"\"\n",
    "        test_embed = self.embed_model.get_text_embedding(text)\n",
    "        column_names = [\"embed_\" + str(i) for i in range(len(test_embed))]\n",
    "        X_test = pd.DataFrame([test_embed], columns=column_names)\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "    # create label method\n",
    "    # This method creates a combined label for a row of the dataset based on individual toxicity indicators.    \n",
    "    def create_label(self, row): \n",
    "        \"\"\"\n",
    "        Creates a combined label for a row of the dataset based on individual toxicity indicators.\n",
    "\n",
    "        Args:\n",
    "        row (Series): A row from the dataset.\n",
    "\n",
    "        Returns:\n",
    "        str: The combined label for the row.\n",
    "        \"\"\"\n",
    "        label = \"|\"\n",
    "        if row[\"toxic\"] == 1:\n",
    "            label += \"toxic|\"\n",
    "        if row[\"severe_toxic\"] == 1:\n",
    "            label += \"severe_toxic|\"\n",
    "        if row[\"obscene\"] == 1:\n",
    "            label += \"obscene|\"\n",
    "        if row[\"threat\"] == 1:\n",
    "            label += \"threat|\"\n",
    "        if row[\"insult\"] == 1:\n",
    "            label += \"insult|\"\n",
    "        if row[\"identity_hate\"] == 1:\n",
    "            label += \"identity_hate|\"\n",
    "        if label == \"|\":\n",
    "            label = \"|non_toxic|\"\n",
    "        return label\n",
    "\n",
    "    # get embeddings method\n",
    "    # This method generates embeddings for a given text using the embedding model.  \n",
    "    def get_embeddings(self, text): \n",
    "        \"\"\"\n",
    "        Generates embeddings for a given text using the embedding model.\n",
    "\n",
    "        Args:\n",
    "        text (str): The text to be embedded.\n",
    "\n",
    "        Returns:\n",
    "        list: The embedding of the text.\n",
    "        \"\"\"\n",
    "        return self.embed_model.get_text_embedding(text=text)\n",
    "\n",
    "    def prepare_data(self, input_path): \n",
    "        \"\"\"\n",
    "        Prepares the training data by reading from a CSV file, creating labels, \n",
    "        and balancing the dataset. The original dataset from here - \n",
    "        https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data\n",
    "\n",
    "        Args:\n",
    "        input_path (str): The path to the input CSV file.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: The prepared training data.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(input_path)\n",
    "        df[\"label\"] = df.progress_apply(self.create_label, axis=1)\n",
    "        toxic_df = df.iloc[np.where(df[\"label\"] != \"|non_toxic|\")]\n",
    "        non_toxic_df = df.iloc[np.where(df[\"label\"] == \"|non_toxic|\")].head(10000)\n",
    "        df = pd.concat([toxic_df, non_toxic_df], ignore_index=True, sort=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7942dd78-441a-4f4e-bd8a-b07208d6de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_rail = Toxic_Rail(mode=\"train\", input_path=\"data/toxic_train.csv\") # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd55c9-d26c-44d9-86d1-98170a81eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.Series(toxic_rail.model.feature_importances_, index=toxic_rail.model.feature_names_in_) # feature importances\n",
    "sig_features = features.sort_values(ascending=False) # significant features\n",
    "sig_features[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1e74f-4e7c-4fe3-92ca-6b87058833c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "toxic_rail.X  # training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dcae7a-59f9-420b-b448-9da1f1f8fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors # for color mapping\n",
    "mcolors.CSS4_COLORS # color palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2357d9-b5bb-49ba-83c0-c894178232a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color mapping\n",
    "colors = list(mcolors.CSS4_COLORS.values())\n",
    "y = toxic_rail.df_train['label'] #\n",
    "keys = y.unique() #\n",
    "color_map = {k:colors[i] for i,k in enumerate(keys)} #\n",
    "color_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed6348-72ea-4560-9db4-8c0f1b6f9550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data along top 2 most important features\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "c_arr = np.array([\"k\"] * len(y))\n",
    "#c_arr = [color_map[l] for l in y] \n",
    "c_arr = [\"#006400\" if(l == \"|non_toxic|\") else \"#CD5C5C\" for l in y]\n",
    "plt.figure(figsize=(8, 6)) #\n",
    "plt.rcParams.update({'font.size': 16}) \n",
    "plt.scatter(toxic_rail.X[sig_features.index[0]], toxic_rail.X[sig_features.index[1]], c=c_arr) #\n",
    "plt.xticks(rotation = 45) #\n",
    "plt.xlabel(sig_features.index[0]) #\n",
    "plt.ylabel(sig_features.index[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c9b08ba-4042-40fa-a107-236acc236c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_rail_pred = Toxic_Rail(mode=\"predict\") # predict mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c55af-2bbf-4cd3-91c9-29b4b85e8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You should be kicked out, you're a moronic wimp who is too lazy to do any work. It makes me sick that people like you exist in this world.\"\n",
    "toxic_rail_pred.predict(text) # predict toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d09c66eb-166d-400f-8a74-af378da30ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['|non_toxic|'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I want to understand how to use the classification models with embeddings to filter out toxic text\"\n",
    "toxic_rail_pred.predict(text) # predict toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a3fc230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['|toxic|'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Raghu hates the people around him and wanted to hit them badly\"\n",
    "toxic_rail_pred.predict(text) # predict toxicity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-kernel",
   "language": "python",
   "name": "my-python3-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
